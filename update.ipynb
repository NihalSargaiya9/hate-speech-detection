{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"update.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LbUZYv41V4u7","executionInfo":{"status":"ok","timestamp":1637524858073,"user_tz":-330,"elapsed":536,"user":{"displayName":"Nihal Sargaiya","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17847363955503734632"}},"outputId":"5f1c999d-0263-49c6-e8d5-bad38b3e468b"},"source":["from gensim import models\n","import numpy as np\n","from numpy.core.fromnumeric import shape\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","from keras.models import Sequential\n","from keras.layers import Dense,Conv1D,MaxPooling1D\n","from keras.layers import LSTM,Dropout\n","from keras.layers import TimeDistributed ,Bidirectional\n","from keras.layers.embeddings import Embedding\n","from keras.preprocessing import sequence\n","from keras.preprocessing.text import Tokenizer\n","import regex as re\n","import pandas as pd\n","from nltk.tokenize import RegexpTokenizer\n","import nltk\n","from gensim.models import Word2Vec\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.datasets import make_blobs\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import classification_report\n","from keras import backend as K\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import average_precision_score\n","nltk.download('punkt')\n"],"execution_count":70,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":70}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zQk4WkqjV7Q4","executionInfo":{"status":"ok","timestamp":1637524858442,"user_tz":-330,"elapsed":11,"user":{"displayName":"Nihal Sargaiya","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17847363955503734632"}},"outputId":"e2be7e65-1f06-4eed-d214-b6437c757ea0"},"source":["data =  pd.read_csv('labeled_data.csv')\n","print(data.size,type(data))\n","labels = data.iloc[::,1]\n","data = data.iloc[::,0]\n","print(data)\n","print(data.size)\n","print(labels.shape[0])\n","noMentionData = []\n","\n","file = open(\"stopwordslist.txt\",\"r\")\n","stopwords_list = []\n","for lines in file:\n","    lines = lines.rstrip(lines[-1])\n","    stopwords_list.append(lines)\n","\n","\n","#Pre-requisites for every model\n","voc_size = 10000\n","# truncate and pad input sequences\n","# max_length = 300\n","# x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n","# x_test = sequence.pad_sequences(x_test, maxlen=max_length)\n"],"execution_count":71,"outputs":[{"output_type":"stream","name":"stdout","text":["49564 <class 'pandas.core.frame.DataFrame'>\n","0        !!!!! RT @mleew17: boy dats cold...tyga dwn ba...\n","1        !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...\n","2        !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...\n","3        !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...\n","4        !!!!!!!!!!!!!!!!!!\"@T_Madison_x: The shit just...\n","                               ...                        \n","24777    you's a muthaf***in lie &#8220;@LifeAsKing: @2...\n","24778    you've gone and broke the wrong heart baby, an...\n","24779    young buck wanna eat!!.. dat nigguh like I ain...\n","24780                youu got wild bitches tellin you lies\n","24781    ~~Ruffled | Ntac Eileen Dahlia - Beautiful col...\n","Name: !!! RT @mayasolovely: As a woman you shouldn't complain about cleaning up your house. &amp; as a man you should always take the trash out..., Length: 24782, dtype: object\n","24782\n","24782\n"]}]},{"cell_type":"code","metadata":{"id":"d3VCPJCmWB_-","executionInfo":{"status":"ok","timestamp":1637524858443,"user_tz":-330,"elapsed":7,"user":{"displayName":"Nihal Sargaiya","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17847363955503734632"}}},"source":["#Preprocessing\n","def preprocess(tweet):  \n","    # removal of @name[mention]\n","    tweet_name = tweet.str.replace('@[\\w\\-]+', '')\n","\n","    # removal of links[https://abc.com]\n","    tweets = tweet_name.str.replace('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]'\n","            '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '')\n","    \n","    # removal of punctuations and numbers\n","    punc_remove = tweets.str.replace(r'[^\\w\\s]',\"\")\n","    # remove whitespace with a single space\n","    newtweet = punc_remove\n","    # remove leading and trailing whitespace\n","    newtweet=newtweet.str.replace(r'^\\s+|\\s+?$','')\n","    # replace normal numbers with numbr\n","    newtweet=newtweet.str.replace(r'\\d+(\\.\\d+)?','numbr')\n","    return newtweet"],"execution_count":72,"outputs":[]},{"cell_type":"code","metadata":{"id":"KEp6qd5sWDjZ","executionInfo":{"status":"ok","timestamp":1637524858443,"user_tz":-330,"elapsed":6,"user":{"displayName":"Nihal Sargaiya","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17847363955503734632"}}},"source":["def getFinalList(newtweet):\n","    final_list=[]\n","    # filter(None, stopwords_list)\n","    for text in newtweet:\n","        words = [word for word in str(text).split() if word.lower() not in stopwords_list]\n","        text = \" \".join(words)\n","        final_list.append(text)\n","    all_words = [nltk.word_tokenize(sent) for sent in final_list]\n","    max_length = 0\n","    for sentence in all_words:\n","        if len(sentence) > max_length:\n","            max_length = len(sentence)\n","    print(max_length)\n","    return all_words\n"],"execution_count":73,"outputs":[]},{"cell_type":"code","metadata":{"id":"FTINUxHxqunO","executionInfo":{"status":"ok","timestamp":1637524858444,"user_tz":-330,"elapsed":7,"user":{"displayName":"Nihal Sargaiya","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17847363955503734632"}}},"source":["def recall_m(y_true, y_pred):\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","    recall = true_positives / (possible_positives + K.epsilon())\n","    return recall\n","\n","def precision_m(y_true, y_pred):\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","    precision = true_positives / (predicted_positives + K.epsilon())\n","    return precision\n","\n","def f1_m(y_true, y_pred):\n","    precision = precision_m(y_true, y_pred)\n","    recall = recall_m(y_true, y_pred)\n","    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n"],"execution_count":74,"outputs":[]},{"cell_type":"code","metadata":{"id":"jsyAObYNWIRS","executionInfo":{"status":"ok","timestamp":1637525352112,"user_tz":-330,"elapsed":400,"user":{"displayName":"Nihal Sargaiya","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17847363955503734632"}}},"source":["def simple_lstm(embedding_layer,voc_size, max_length, x_train, y_train, x_test, y_test):\n","    \n","    #Word vector matrix\n","    embedding_vector_features=32\n","    #Defining a sequential model\n","    model=Sequential()\n","    \n","    \n","    #Defining input and Embedding layer\n","    # model.add(Embedding(input_dim=x_train.syn0.shape[0], output_dim=model.syn0.shape[1], weights=[model.syn0]))\n","    model.add(embedding_layer)\n","   \n","    #1st LSTM hidden layer with 128 neurons and ReLu activation function\n","    model.add(LSTM(64,activation='sigmoid',return_sequences=True))\n","    #Dropout layer with 20% dropout frequency\n","    model.add(Dropout(0.5))\n","    #2nd LSTM hidden layer with 32 neurons and ReLu activation function\n","    # model.add(LSTM(32,activation='relu'))\n","    #Dropout layer with 20% dropout frequency\n","    # model.add(Dropout(0.2))\n","    #3rd Dense hidden layer with 4 neurons and ReLu activation function\n","    # model.add(Dense(4,activation='relu'))\n","    #Dropout layer with 20% dropout frequency\n","    # model.add(Dropout(0.2))\n","    #Output Layer with 1 neuron for 2 different categories and sigmoid activation function\n","    # model.add(Dense(1,activation='sigmoid'))\n","    model.add(TimeDistributed(Dense(1, activation='sigmoid')))\n","    #Compiling the model\n","    \n","    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['acc'])\n","    #model.build()\n"," \n","    \n","    #Will print the summary of the model\n","    \n","    \n"," \n","    print(model.summary())\n","    print(\"x_train: \",x_train.shape)\n","    print(\"y_train: \",y_train.shape)\n","    #Training the model\n","    model.fit(x_train,y_train,epochs=1,batch_size=256)\n","    #Evaluating the model\n","    accuracy= model.evaluate(x_test, y_test, verbose=0)\n","    # accuracy = model.evaluate(x_test, y_test)\n","    #Printing the accuracy\n","    # y_pred = model.predict(x_test).round()\n","    y_pred = model.predict_on_batch(x_test).round().argmax(1)\n","    print(y_pred.shape,y_test.shape)\n","    print('Accuracy: %.2f' % (accuracy[1]*100))\n","    print('F1 score: %.2f' % f1_score(y_test, y_pred, average='micro'))\n","    print('Precision: %.2f' % average_precision_score(y_test, y_pred, average=\"micro\"))\n","    return model\n","    "],"execution_count":88,"outputs":[]},{"cell_type":"code","metadata":{"id":"BMr3aC_TasEU","executionInfo":{"status":"ok","timestamp":1637524859143,"user_tz":-330,"elapsed":3,"user":{"displayName":"Nihal Sargaiya","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17847363955503734632"}}},"source":["#CNN LSTM\n","def cnn_lstm_model(embedding_layer,x_train, y_train, x_test, y_test):\n","    # define the sequential model\n","    model = Sequential()\n","    \n","    model.add(embedding_layer)\n","    model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n","    model.add(MaxPooling1D(pool_size=2))\n","    model.add(LSTM(350))\n","    model.add(Dense(1, activation='sigmoid'))\n","\n","\n","    # compile the model\n","    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n","    model.build()\n","    #Will print the summary of the model\n","    print(model.summary())\n","    \n","    # fit the model on the dataset\n","    model.fit(x_train, y_train, epochs=10, batch_size=128)\n","    # evaluate the model\n","    accuracy = model.evaluate(x_test, y_test, verbose=0)\n","    # accuracy = model.evaluate(x_test, y_test)\n","    # Printing the accuracy\n","    # print('Loss: %.2f' % (loss))\n","\n","    y_pred = model.predict(x_test).round()\n","    print('Accuracy: %.2f' % (accuracy[1]*100))\n","    print('F1 score: %.2f' % f1_score(y_test, y_pred, average='micro'))\n","    print('Precision: %.2f' % average_precision_score(y_test, y_pred, average=\"micro\"))\n","    # print('Recall: %.2f' % (recall))\n","   # Confusion Matrix\n","    # print(y_pred, y_pred.round())\n","    # cfm = confusion_matrix(y_test, y_pred.round())\n","    # print(cfm)\n","    # y_pred_bool = np.argmax(y_pred.round(), axis=1)\n","    # print(classification_report(y_test, y_pred_bool))\n","    \n","    return model\n","\n"],"execution_count":76,"outputs":[]},{"cell_type":"code","metadata":{"id":"uVxjMJEJ9nyR","executionInfo":{"status":"ok","timestamp":1637525171990,"user_tz":-330,"elapsed":513,"user":{"displayName":"Nihal Sargaiya","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17847363955503734632"}}},"source":["#Simple Neural Network\n","def cnn_model(embedding_layer,x_train, y_train, x_test, y_test):\n","    # define the sequential model\n","    model = Sequential()\n","    \n","    model.add(embedding_layer)\n","    model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n","    model.add(MaxPooling1D(pool_size=2))\n","    model.add(Dense(32, activation='relu'))\n","    model.add(Dropout(0.2))\n","    model.add(Dense(1, activation='sigmoid'))\n","\n","\n","    # compile the model\n","    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n","    model.build()\n","    #Will print the summary of the model\n","    print(model.summary())\n","    \n","    # fit the model on the dataset\n","    model.fit(x_train, y_train, epochs=20, batch_size=128)\n","    # evaluate the model\n","    # evaluate the model\n","    accuracy= model.evaluate(x_test, y_test, verbose=0)\n","    # accuracy = model.evaluate(x_test, y_test)\n","    #Printing the accuracy\n","    y_pred = model.predict_on_batch(x_test).round().argmax(1)\n","    ny =[]\n","    for x in y_pred:\n","      ny.append([x[0]])\n","    print(ny)\n","    print(\"================\\n\\n\",y_pred,\"=================\\n\\n\",y_pred.shape,y_test.shape,y_test)\n","    print('Accuracy: %.2f' % (accuracy[1]*100))\n","    print('F1 score: %.2f' % f1_score(y_test, y_pred, average='micro'))\n","    print('Precision: %.2f' % average_precision_score(y_test, y_pred, average=\"micro\"))\n","    return model"],"execution_count":86,"outputs":[]},{"cell_type":"code","metadata":{"id":"EGJXkbdhWK7w","executionInfo":{"status":"ok","timestamp":1637524859143,"user_tz":-330,"elapsed":3,"user":{"displayName":"Nihal Sargaiya","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17847363955503734632"}}},"source":["#Simple Neural Network\n","def simple_Neural_network(x_train, y_train, max_length):\n","    # define the sequential model\n","    model = Sequential()\n","    \n","    #Adding input layer and first hidden layer with 128 neurons and ReLu activation function\n","    model.add(Dense(128, input_dim=max_length, activation='relu'))\n","    model.add(Dropout(0.2))\n","    #2nd hidden layer with 32 neurons and ReLu activation function\n","    model.add(Dense(64, activation='relu'))\n","    model.add(Dropout(0.2))\n","    #3rd hidden layer with 4 neurons and ReLu activation function\n","    model.add(Dense(32, activation='relu'))\n","    #Output layer with 1 neuron and sigmoid activation function\n","    model.add(Dense(1, activation='sigmoid'))\n","    \n","    # compile the model\n","    model.compile(loss='binary_crossentropy', optimizer='RMSprop', metrics=['acc'])\n","    model.build()\n","    #Will print the summary of the model\n","    print(model.summary())\n","    \n","    # fit the model on the dataset\n","    model.fit(x_train, y_train, epochs=80, batch_size=32)\n","    # evaluate the model\n","    # evaluate the model\n","    accuracy = model.evaluate(x_test, y_test, verbose=0)\n","    # accuracy = model.evaluate(x_test, y_test)\n","    #Printing the accuracy\n","    y_pred = model.predict(x_test).round()\n","    print('Accuracy: %.2f' % (accuracy[1]*100))\n","    print('F1 score: %.2f' % f1_score(y_test, y_pred, average='micro'))\n","    print('Precision: %.2f' % average_precision_score(y_test, y_pred, average=\"micro\"))\n","    return model"],"execution_count":78,"outputs":[]},{"cell_type":"code","metadata":{"id":"BtURu1EQWPJD","executionInfo":{"status":"ok","timestamp":1637524859143,"user_tz":-330,"elapsed":2,"user":{"displayName":"Nihal Sargaiya","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17847363955503734632"}}},"source":["#  newtweet = preprocess(data)\n","# all_words = getFinalList(newtweet)\n","# # print(all_words)\n","# w2v_model = Word2Vec(size=32, window=4, min_count=2, workers=8)\n","# w2v_model.build_vocab(all_words)\n","# words = w2v_model.wv.vocab.keys()\n","# vocab_size = len(words)\n","# # print(\"Vocab size\", vocab_size)\n","\n","# w2v_model.train(all_words, total_examples=len(all_words), epochs=7)\n","# print(w2v_model.wv.most_similar(\"love\"))\n","\n","\n","# tokenizer = Tokenizer()\n","# # print(newtweet.iloc[0])\n","# # print(newtweet[0])\n","# newtweet=newtweet.astype(str)\n","# tokenizer.fit_on_texts(newtweet)\n","# vocab_size = len(tokenizer.word_index) + 1\n","# print(\"Total words\", vocab_size)\n","\n","# x_train, x_test, y_train, y_test = train_test_split(newtweet, labels, test_size=0.33, random_state=42)\n","# x_train = sequence.pad_sequences(tokenizer.texts_to_sequences(x_train), maxlen=300)\n","# x_test = sequence.pad_sequences(tokenizer.texts_to_sequences(x_test), maxlen=300)\n","\n","\n","# encoder = LabelEncoder()\n","# encoder.fit(labels.tolist())\n","\n","# y_train = encoder.transform(y_train.tolist())\n","# y_test = encoder.transform(y_test.tolist())\n","\n","# y_train = y_train.reshape(-1,1)\n","# y_test = y_test.reshape(-1,1)\n","# print(\"x_train\", x_train.shape)\n","# print(\"x_test\", x_test.shape)\n","# print(\"y_train\",y_train.shape)\n","# print(\"y_test\",y_test.shape)\n","\n","# embedding_matrix = np.zeros((vocab_size, 32))\n","# for word, i in tokenizer.word_index.items():\n","#   if word in w2v_model.wv:\n","#     embedding_matrix[i] = w2v_model.wv[word]\n","# print(embedding_matrix.shape)\n","\n","# embedding_layer = Embedding(vocab_size, 32, weights=[embedding_matrix], input_length=300, trainable=False)\n","# max_length=300\n","# #Simple LSTM Model\n","\n","# # model=simple_lstm(embedding_layer,voc_size, max_length, x_train, y_train, x_test, y_test)\n","# # model=simple_Neural_network(x_train, y_train, max_length)\n","# model = cnn_lstm_model(embedding_layer,x_train, y_train, x_test, y_test)\n","# # model=cnn_model(embedding_layer,x_train, y_train, x_test, y_test)\n","\n"],"execution_count":79,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xg_8GTxg8VIT","executionInfo":{"status":"ok","timestamp":1637525478642,"user_tz":-330,"elapsed":110510,"user":{"displayName":"Nihal Sargaiya","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17847363955503734632"}},"outputId":"86903f72-9ba9-44ef-e438-71ac29226cb6"},"source":[" newtweet = preprocess(data)\n","all_words = getFinalList(newtweet)\n","# print(all_words)\n","w2v_model = Word2Vec(size=32, window=4, min_count=2, workers=8)\n","w2v_model.build_vocab(all_words)\n","words = w2v_model.wv.vocab.keys()\n","vocab_size = len(words)\n","# print(\"Vocab size\", vocab_size)\n","\n","w2v_model.train(all_words, total_examples=len(all_words), epochs=7)\n","print(w2v_model.wv.most_similar(\"love\"))\n","\n","\n","tokenizer = Tokenizer()\n","# print(newtweet.iloc[0])\n","# print(newtweet[0])\n","newtweet=newtweet.astype(str)\n","tokenizer.fit_on_texts(newtweet)\n","vocab_size = len(tokenizer.word_index) + 1\n","print(\"Total words\", vocab_size)\n","\n","x_train, x_test, y_train, y_test = train_test_split(newtweet, labels, test_size=0.33, random_state=42)\n","x_train = sequence.pad_sequences(tokenizer.texts_to_sequences(x_train), maxlen=300)\n","x_test = sequence.pad_sequences(tokenizer.texts_to_sequences(x_test), maxlen=300)\n","\n","\n","encoder = LabelEncoder()\n","encoder.fit(labels.tolist())\n","\n","y_train = encoder.transform(y_train.tolist())\n","y_test = encoder.transform(y_test.tolist())\n","\n","y_train = y_train.reshape(-1,1)\n","y_test = y_test.reshape(-1,1)\n","print(\"x_train\", x_train.shape)\n","print(\"x_test\", x_test.shape)\n","print(\"y_train\",y_train.shape)\n","print(\"y_test\",y_test.shape)\n","\n","embedding_matrix = np.zeros((vocab_size, 32))\n","for word, i in tokenizer.word_index.items():\n","  if word in w2v_model.wv:\n","    embedding_matrix[i] = w2v_model.wv[word]\n","print(embedding_matrix.shape)\n","\n","embedding_layer = Embedding(vocab_size, 32, weights=[embedding_matrix], input_length=300, trainable=False)\n","max_length=300\n","#Simple LSTM Model\n","\n","model=simple_lstm(embedding_layer,voc_size, max_length, x_train, y_train, x_test, y_test)\n","# model=simple_Neural_network(x_train, y_train, max_length)\n","# model = cnn_lstm_model(embedding_layer,x_train, y_train, x_test, y_test)\n","# model=cnn_model(embedding_layer,x_train, y_train, x_test, y_test)\n","\n"],"execution_count":89,"outputs":[{"output_type":"stream","name":"stdout","text":["26\n","[('loyal', 0.9950183033943176), ('numbr', 0.9941142201423645), ('Yall', 0.9938414096832275), ('trust', 0.9898370504379272), ('Side', 0.9898181557655334), ('em', 0.9891805052757263), ('man', 0.9890669584274292), ('dat', 0.9887652397155762), ('dick', 0.9881954193115234), ('ah', 0.9876891374588013)]\n","Total words 26031\n","x_train (16603, 300)\n","x_test (8179, 300)\n","y_train (16603, 1)\n","y_test (8179, 1)\n","(26031, 32)\n","WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","Model: \"sequential_20\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_20 (Embedding)    (None, 300, 32)           832992    \n","                                                                 \n"," lstm_6 (LSTM)               (None, 300, 64)           24832     \n","                                                                 \n"," dropout_17 (Dropout)        (None, 300, 64)           0         \n","                                                                 \n"," time_distributed_3 (TimeDis  (None, 300, 1)           65        \n"," tributed)                                                       \n","                                                                 \n","=================================================================\n","Total params: 857,889\n","Trainable params: 24,897\n","Non-trainable params: 832,992\n","_________________________________________________________________\n","None\n","x_train:  (16603, 300)\n","y_train:  (16603, 1)\n","65/65 [==============================] - 56s 835ms/step - loss: 0.5233 - acc: 0.7765\n","(8179, 1) (8179, 1)\n","Accuracy: 83.07\n","F1 score: 0.17\n","Precision: 0.83\n"]}]},{"cell_type":"code","metadata":{"id":"d_b6w1jbViuy","executionInfo":{"status":"aborted","timestamp":1637524938088,"user_tz":-330,"elapsed":394,"user":{"displayName":"Nihal Sargaiya","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17847363955503734632"}}},"source":["# data =  pd.DataFrame([\"happy\",\"bitch\",\"best\",\"lol\",\"Preeti is good\",\"This is not offensive\",\"no offence\",\"black bitch\",\"anay\"])\n","# data =  pd.read_csv('pred.csv')\n","# rawdata = list(data.iloc[::,0])\n","# data = data.iloc[::,0]\n","# noMentionData = []\n","# file = open(\"stopwordslist.txt\",\"r\")\n","# stopwords_list = []\n","# for lines in file:\n","#     lines = lines.rstrip(lines[-1])\n","#     stopwords_list.append(lines)\n","# newtweet = preprocess(data)\n","# all_words = getFinalList(newtweet)\n","\n","# tokenizer = Tokenizer()\n","\n","# newtweet=newtweet.astype(str)\n","# tokenizer.fit_on_texts(newtweet)\n","\n","# data = sequence.pad_sequences(tokenizer.texts_to_sequences(data), maxlen=300)\n","\n","\n","# yans = (model.predict(data).round())\n","# print(rawdata)\n","# for x in range(yans.shape[0]):\n","#   print(rawdata[x],yans[x])\n","  # print()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XEqH1gq5oRh4"},"source":["# New Section"]},{"cell_type":"markdown","metadata":{"id":"cGEyoZYuHM7t"},"source":["# New Section"]}]}